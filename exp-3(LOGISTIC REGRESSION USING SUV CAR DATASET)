import pandas as pd
import numpy as np
from numpy import log, dot, exp, shape
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score
data = pd.read_csv('suv_data.csv')
x = data.iloc[:, [2, 3]].values
y = data.iloc[:, 4].values
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=0)
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))
def F1_score(y, y_hat):
    tp = sum((y == 1) & (y_hat == 1))
    fp = sum((y == 0) & (y_hat == 1))
    fn = sum((y == 1) & (y_hat == 0))
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    return 2 * precision * recall / (precision + recall)
class LogisticRegressionCustom:
    def sigmoid(self, z): return 1 / (1 + np.exp(-z))
    def fit(self, X, y, alpha=0.001, iterations=400):
        X = np.c_[np.ones((X.shape[0], 1)), X]
        self.weights = np.zeros((X.shape[1], 1))
        for _ in range(iterations):
            self.weights -= alpha * X.T.dot(self.sigmoid(X.dot(self.weights)) - y.reshape(-1, 1))
    def predict(self, X):
        X = np.c_[np.ones((X.shape[0], 1)), X]
        return (self.sigmoid(X.dot(self.weights)) > 0.5).astype(int).flatten()
def standardize(X):
    for i in range(shape(X)[1]):
        X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])
standardize(x_train)
standardize(x_test)
model = LogisticRegressionCustom()
model.fit(x_train, y_train)
y_pred_test = model.predict(x_test)
y_pred_train = model.predict(x_train)
print(F1_score(y_train, y_pred_train))
print(F1_score(y_test, y_pred_test))
print(confusion_matrix(y_test, y_pred_test))
print(accuracy_score(y_test, y_pred_test))
